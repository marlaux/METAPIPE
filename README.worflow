##ATTENTION!!! The detailed guiding and description are in each step folder!!!! This is just an introduction:

The main purpose of this repository is to help structure and organize the Metabarcoding pipeline, with special care to build scripts as user friendly as possible. 

The scripts are presented as a chain of commands in step by step mode, along with wrapped scripts formatted to run in job queue or interactively using srun (scripts with flags).   
You can choose to run command by command, using independent little scripts, or using the wrapped scripts (named with "_wrap.sh"), or using the flag scripts, with arguments in 
command line, allowing the user to view a help page and select the inputs dinamically (named with "_auto.sh").

You can allocate resources to work in command line using srun:
srun --ntasks=1 --mem-per-cpu=8G --time=02:00:00 --qos=devel --account=nn9813k --pty bash -i
The maximum time is 2 hours and if you know you need more RAM memory, change to 16G (--mem-per-cpu=16G)
There is a specific chain of scripts planned to run as slurm job scripts throughout the entire pipeline. 
The mode of use of each script is described in each README step (e.g. README.demulti). 

The step by step mode, on the other hand, runs each step and each script one after another. It is a good choice when your goal is to have a closer look to your outputs
and what is been done in each step. The step by step mode takes a longer time to run, because for each job you go back to the queue, but provides a deeper knowledge about the 
Metabarcoding data processing, since the user must know not only the inputs and outputs, but also de intermediary files generated throughout the pipeline and why.

#STEP BY STEP WORKFLOW using queue system (recommended)
#############
###1_merge###
#############
1.edit the script to run in queue, type:
vi merge_pear.sh
    INPUT_F="training_set_R1_subsampled.fastq"
    INPUT_R="training_set_R2_subsampled.fastq"
    THREADS="8"
    PVALUE="0.001"  
    OVERLAP="20"     
    QUAL="20"       
    UNCALLED="0"	#do not allow N's
    OUTPUT="my_training_set"
save and quit typing:
esc :wq!

2.edit (or only verify) the slurm file:
vi run_merge_pear.slurm
    #!/bin/bash
    #SBATCH --account=nn9813k
    #SBATCH --job-name=Pear
    #SBATCH --ntasks=8
    #SBATCH --cpus-per-task=1
    #SBATCH --ntasks-per-node=8
    #SBATCH --mem-per-cpu=8G 
    #SBATCH --time=72:00:00

    module --quiet purge
    module load StdEnv
    module load PEAR/0.9.11-GCCcore-9.3.0

    set -o errexit 
    set -o nounset 

    ./merge_pear.sh
    
save and quit:
esc :wq!

run:
sbatch run_merge_pear.slurm
squeue -u user

###############
###2_demulti###
###############
1. First you need to format the barcodes/mapping file text (tags -> sample) to a fasta file:
type:
perl preparing_tags_LCPI.pl 
    #####DEMULTIPLEXING DUAL INDEXED LIBRARIES#####
    EXPECTED INPUT:
    sample1 tagF    tagR
    sample2 tagF    tagR
    ...     #same as in your excel file
    ##DO NOT INCLUDE PRIMERS NOW

    Please, enter your mapping file to edit: (copy and paste:)my_mapping_file.txt
    >>>For Illumina merged reads type 'linked'
    >>>For Illumina combinatorial type 'combinatorial'
    >>>For Illumina exact paired dual index type 'unique'
    >>>For Ion torrent dual index type 'ion'
    >>> (type linked)

The output will be the same input mapping file, but in fasta format:
input:
head my_mapping_file.txt
    1Ax      AGGTACGCAATT    CCTAAACTACGG
    2Ax      ACAGCCACCCAT    CCTAAACTACGG
    3Ax      TGTCTCGCAAGC    CCTAAACTACGG
    ...
the output will be:
head Barcodes_LA1.fa 
    >1A
    ^AGGTACGCAATT...CCTAAACTACGG$
    >2A
    ^ACAGCCACCCAT...CCTAAACTACGG$
    >3A
    ^TGTCTCGCAAGC...CCTAAACTACGG$
(the same for LA2 and LA3, the reverse complement variations for the linked mode)
***files in cutadapt format

2. Running the demultiplexing:
vi demulti_dual_index_linked.sh	
edit only this:
    INPUT="its_illumina.assembled.fastq" --> OUTPUT FROM 1_merge STEP!!!
    PAIR1="Barcodes_LA1.fa"
    PAIR2="Barcodes_LA2.fa"
    PAIR3="Barcodes_LA3.fa"
    
The output samples will be in "demulti_linked" folder.
This step only works on SAGA job queue! Too heavy to run in bash, even using srun!
3. edit and verify the slurm file:
vi run_demulti_cutadapt.slurm
uncomment ONLY the script you need:
    #./demulti_dual_index_ion.sh
    ./demulti_dual_index_linked.sh (this one)
    #./demulti_dual_index_combinatorial.sh
    #./demulti_unique_dual_index.sh
    #./tags_primers_clip_trim.sh
save and quit:
esc :wq!

run:
sbatch run_primer_clipping.slurm
squeue -u user

#####################
###3_read_cleaning###
#####################
complete read cleaning from tags and primers clipping to dereplicated samples:
1. copy the mapping file in fasta fromat used to demultiplex:
cp ../demulti_linked/Bracodes_LA* .
Barcodes_LA1.fa, Barcodes_LA2.fa, Barcodes_LA3.fa

2. edit the address for your demultiplexed samples inside the first script, type:
vi tag_primer_clipping_wrap1.sh
edit:
PRIMER_F1="TGTGAATTGCARRATYCMG"
PRIMER_R1="CCCGHYTGAYYTGRGGTCD"
PRIMER_F1_RC="CKGRATYYTGCAATTCACA"
PRIMER_R1_RC="HGACCYCARRTCARDCGGG" --> **You can get your primers reverse complement in this website: http://arep.med.harvard.edu/labgc/adnan/projects/Utilities/revcomp.html
then edit only the address:
    for file in /cluster/projects/nn9813k/metapipe/ITS/ITS_illumina/demulti_linked/*_LA.fq;
save and quit:
esc :wq!
the output will be in /adapter_clip and /primer_clip folders

3. edit the slurm file, type:
vi run_primer_clipping.slurm
uncomment only the wrap1:
    ./tag_primer_clipping_wrap1.sh 
    #./tag_primer_clipping_wrap2.sh
save and quit:
esc :wq!

run:
sbatch run_primer_clipping.slurm
squeue -u user

4. edit only the address for the adapters and primers clipped samples inside the second script, type:
vi tag_primer_clipping_wrap2.sh
edit address:
  for file in /cluster/projects/nn9813k/metapipe/ITS/ITS_illumina/clipping_ML/primer_clip/*_trim2.fq;
save and quit:
esc :wq!

5. edit the slurm file, type:
vi run_primer_clipping.slurm
uncomment only the wrap2 and comment the wrap1:
    #./tag_primer_clipping_wrap1.sh 
    ./tag_primer_clipping_wrap2.sh
save and quit:
esc :wq!

run:
sbatch run_primer_clipping.slurm
squeue -u user

the output will be in /trimming and /dereplicated folders

##################
###5_clustering###
##################
1. the following wrapped script do all the job:
vi clustering_swarm_with_chim_filter_wrap.sh
edit:
    GLOBAL_DP="my_project_global_dp.fas"
    LENGTH=400 ###according to your expected amplicon lenght
and right below, the address to the dereplicated samples:
# Global dereplication
    cat /cluster/projects/nn9813k/metapipe/ITS/dereplicated/*_dp.fasta > "${TMP_FASTA}"
save and quit:
esc :wq!

2. verify the slurm file:
vi run_cluster_swarm.slurm
#!/bin/bash
#SBATCH --job-name=cluster
#SBATCH --time=72:00:00
#SBATCH --account=nn9813k
#SBATCH --ntasks=8
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-node=8
#SBATCH --mem-per-cpu=16G

set -o errexit
set -o nounset

./clustering_swarm_with_chim_filter_wrap.sh

save and quit:
esc :wq!

run:
sbatch run_cluster_swarm.slurm
squeue -u user

the output will be in the current folder, the most important are:
my_project_global_dp.fas
my_project_global_dp_2f_representatives.fas --> input for taxa assignment
my_project_global_dp.swarms
my_project_global_dp.stats
my_project_global_dp.struct
my_project_global_dp.uchime
my_project_global_dp_nonchimeras_sorted.fasta --> optional input for taxa assignment

they are the input for build the OTU table, together with the quality file generated in step 3 (my_project_quality_file.qual)

###################
###6_taxa_assign###
###################
FOR NCBI LOCAL ALIGNMENT STRATEGIES
1 ./EntrezDirect.sh -h		in: my query  out: my_search_esearch.fasta
##################################################
Download reference sequences in fasta format.
Entrez Direct NCBI command line tools

Usage: ./EntrezDirect.sh [-d nuccore|protein] [-q gene] [-t taxa] [-o output]
-d     NCBI database: 'nuccore' (nucleotide) or 'protein'
-q     Entrez search terms: 'COI [gene]' or 'Internal transcribed spacer'
-t     Expected taxonomic target: 'Arthropoda [ORGN]' or 'plants [ORGN]'
-o     Name for the local database.
-h     Print this Help.
##################################################

2 ./format_NCBI2lineages_auto.sh -h 	in: references downloaded  out: my_refs.uniq.fasta
##################################################
Format and include taxonomic lineage in reference
sequences headers downloaded from NCBI taxonomy.
This script will generate a multifasta file which
will be the input for makeblastdb to build your
local database to run in BLAST.

Usage: ./format_NCBI2lineages_auto.sh [-r fasta] [-p output_prefix]
-r     references_downloaded_fromNCBI.fasta
-p     prefix to output filenames
-h     Print this Help.
##################################################

3 BLAST.sh --> script working, but not final --> in: OTUs from clustering step    out: Best hit BLAST table
in: _2f_representatives.fas/my_refs.uniq.fasta		out: your_project_local_blast.tab

4 perl Blast_results_format.pl --> format BLAST output to work in OTU table and stampadapt.sh
in: your_project_local_blast.tab		out: Blast_formatted2OTUtable.tab

###7_build_OTU_table###
vi build_OTU_table.sh
in: global derep fasta, clustering outputs, Blast_formatted2OTUtable.tab and quality file
out: the complete version of the OTU table.

###8_filter_table_phyloseq###
1. filter_OTU_table.sh   in: OTU table   out: OTU table formatted to load in Phyloseq
2. perl taxa_assignment_table.pl Blast_formatted2OTUtable.tab  out: final_OTU_tax_assignments.txt to load in Phyloseq
3. script for loading your tables in phyloseq and some first steps: metapipe_phyloseq.R

Second taxa assignment option:
###6_taxa_assign###
1 primers_cut.sh : Download your references from BOLD, cut it with your primers and format header with taxonomic lineage

2 ./stampadapt.sh -h
##################################################
Runs the adapted version of STAMPA LCA taxa assignment
Usage: ./stampadapt.sh [-q query.fasta] [-d references.fasta] [-o output] [-p pwd]
-q     OTUs fasta file, the output from clustering step
-d     references sequences formatted
-o     output name for the final assignments file.
-h     print this help
##################################################
3 stampaplot.sh : Just one example of how to plot STAMPA results, still in progress

