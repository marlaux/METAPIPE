##ATTENTION!!! The detailed guiding and description are in each step folder!!!! This is just an introduction:

The main purpose of this repository is to help structure and organize the Metabarcoding pipeline, with special care to build scripts as user friendly as possible. 

The scripts are presented as a chain of commands in step by step mode, along with wrapped scripts formatted to run in job queue or interactively using srun (scripts with flags).   
You can choose to run command by command, using independent little scripts, or using the wrapped scripts (named with "_wrap.sh"), or using arguments in command line, allowing the
user to view a help page and select the inputs dinamically (named with "_auto.sh").

You can allocate resources to work in command line using srun:
srun --ntasks=1 --mem-per-cpu=8G --time=02:00:00 --qos=devel --account=nn9813k --pty bash -i
The maximum time is 2 hours and if you know you need more RAM memory, change to 16G (--mem-per-cpu=16G)
There is a specific chain of scripts planned to run as slurm job scripts throughout the entire pipeline. 
The mode of use of each script is described in each README step (e.g. README.demulti). 

The step by step mode, on the other hand, runs each step and each script one after another. It is a good choice when your goal is to have a closer look to your outputs
and what is been done in each step. The step by step mode takes a longer time to run, because for each job you go back to the queue, but provides a deeper knowledge about the 
Metabarcoding data processing, since the user must know not only the inputs and outputs, but also de intermediary files generated throughout the pipeline and why.

#STEP BY STEP WORKFLOW using queue system (recommended)
#############
###1_merge###
#############
1.edit the script to run in queue, type:
*vi merge_pear.sh*
INPUT_F="training_set_R1_subsampled.fastq"
INPUT_R="training_set_R2_subsampled.fastq"
THREADS="8"
PVALUE="0.001"  
OVERLAP="20"     
QUAL="20"       
UNCALLED="0"	#do not allow N's
OUTPUT="my_training_set"
save and quit typing:
type esc :wq!

2.edit (or only verify) the slurm file:
vi run_merge_pear.slurm
#!/bin/bash
#SBATCH --account=nn9813k
#SBATCH --job-name=Pear
#SBATCH --ntasks=8
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-node=8
#SBATCH --mem-per-cpu=8G 
#SBATCH --time=72:00:00

module --quiet purge
module load StdEnv
module load PEAR/0.9.11-GCCcore-9.3.0

set -o errexit 
set -o nounset 

./merge_pear.sh

###############
###2_demulti###
###############
1. First you need to format the barcodes/mapping file text (tags -> sample) to a fasta file:
type:
perl preparing_tags_LCPI.pl 
#####DEMULTIPLEXING DUAL INDEXED LIBRARIES#####
EXPECTED INPUT:
sample1 tagF    tagR
sample2 tagF    tagR
...     #same as in your excel file
##DO NOT INCLUDE PRIMERS NOW

Please, enter your mapping file to edit: (copy and paste:)my_mapping_file.txt
>>>For Illumina merged reads type 'linked'
>>>For Illumina combinatorial type 'combinatorial'
>>>For Illumina exact paired dual index type 'unique'
>>>For Ion torrent dual index type 'ion'
>>> (type linked)

The output will be the same input mapping file, but in fasta format:
input:
head my_mapping_file.txt
1Ax      AGGTACGCAATT    CCTAAACTACGG
2Ax      ACAGCCACCCAT    CCTAAACTACGG
3Ax      TGTCTCGCAAGC    CCTAAACTACGG
...
the output will be:
head Barcodes_LA1.fa 
>1A
^AGGTACGCAATT...CCTAAACTACGG$
>2A
^ACAGCCACCCAT...CCTAAACTACGG$
>3A
^TGTCTCGCAAGC...CCTAAACTACGG$
(the same for LA2 and LA3, the reverse complement variations for the linked mode)
***files in cutadapt format

2. Running the demultiplexing:
vi demulti_dual_index_linked.sh	
edit only this:
INPUT="its_illumina.assembled.fastq" --> output from 1_merge step
PAIR1="Barcodes_LA1.fa"
PAIR2="Barcodes_LA2.fa"
PAIR3="Barcodes_LA3.fa"
The output samples will be in "demulti_linked" folder.
This step only works on SAGA job queue! Too heavy to run in bash, even using srun!
vi run_demulti_cutadapt.slurm
uncomment ONLY the script you need:
#./demulti_dual_index_ion.sh
./demulti_dual_index_linked.sh (this one)
#./demulti_dual_index_combinatorial.sh
#./demulti_unique_dual_index.sh
#./tags_primers_clip_trim.sh


###3_read_cleaning###
complete read cleaning from tags and primers clipping to dereplicated samples:
./tags_primers_clip_trim_auto.sh -h
##################################################
Remove tags and primers from demultiplexed sample files and processes a series of format and cleaning steps, generating the quality file necessary for clustering, the fasta converted sample files and dereplicated by sample files..
You can get your primers reverse complement in this website:
http://arep.med.harvard.edu/labgc/adnan/projects/Utilities/revcomp.html

Usage: ./tags_primers_clip_trim.sh [-F primerF] [-R primerR] [-f RCprimerF] [-r RCprimerR] [-l 50] [-p pwd] [-e 0.2] [-b barcode.txt] [-q name] [-t threads]
-F     paste your primerF
-R     paste your primerR
-f     paste your reverse complement primerF
-r     paste your reverse complement primerR
-l     minimum length after trimming
-p     complete directory where the demultiplexed samples are
-e     maximum error rate allowed, higher more flexible
-b     barcodes.txt, the original mapping file used as input for perl preparing_tags_LCPI.pl script.
-t     number of threads
-h     Print this Help
##################################################

Script by script:
1.tag_primer_clipping_part1.sh	in: demultiplexed fastq samples	out: clipped and trimmed tags and primers
2.read_trim.sh	in: clipped fastq samples	out: trimmed and header formatted fastq		
3.convert2fasta.sh	in: clip/formatted fastq	out: clip/formatted fasta		
4.extract_quality.sh	in: clip/formatted fastq 	out: quality text with error rate and length
5.dereplicate_by_sample.sh	in: clip/formatted fasta 	out: dereplicated samples
>>>>Edit tag_primer_clipping_part1.sh and tag_primer_clipping_part1.sh to run sequentially in SAGA slurm job

###4_dereplicate###
edit derep_sample_global.sh and run in slurm job

###5_clustering###
clustering step, complete chain, in: dereplicated samples	out: representatives/struct/swarms/uchime and concat global derep multifasta
./clustering_swarm_wrap.sh -h
##################################################
Global dereplication, clustering using SWARM, formatting and chimera searching wrapped script.

Usage: ./clustering_swarm_wrap.sh [-l length] [-o output] [-p pwd]
-l     expected amplicon lenght
-o     output name 'my_project_marker'
-p     complete path to dereplicated by samples directory (3_read_cleaning step, derep_sample)
-h     print this help
##################################################

script by script:
1.global_dereplication.sh	in: dereplicated samples	out: concatenated globally dereplicated multifasta
2.clustering_swarm.sh	in: concatenated globally dereplicated multifasta	out: representatives/struct/swarms/uchime
3.sort_representatives.sh	in: _1f_representatives.fas		out: _2f_representatives.fas
4.chimera_checking.sh	in: _1f_representatives.fas		out: in: _1f_representatives.uchime		
>>>clustering_swarm_complete.sh	to run using slurm job.

###6_taxa_assign###
FOR NCBI LOCAL ALIGNMENT STRATEGIES
1 ./EntrezDirect.sh -h		in: my query  out: my_search_esearch.fasta
##################################################
Download reference sequences in fasta format.
Entrez Direct NCBI command line tools

Usage: ./EntrezDirect.sh [-d nuccore|protein] [-q gene] [-t taxa] [-o output]
-d     NCBI database: 'nuccore' (nucleotide) or 'protein'
-q     Entrez search terms: 'COI [gene]' or 'Internal transcribed spacer'
-t     Expected taxonomic target: 'Arthropoda [ORGN]' or 'plants [ORGN]'
-o     Name for the local database.
-h     Print this Help.
##################################################

2 ./format_NCBI2lineages_auto.sh -h 	in: references downloaded  out: my_refs.uniq.fasta
##################################################
Format and include taxonomic lineage in reference
sequences headers downloaded from NCBI taxonomy.
This script will generate a multifasta file which
will be the input for makeblastdb to build your
local database to run in BLAST.

Usage: ./format_NCBI2lineages_auto.sh [-r fasta] [-p output_prefix]
-r     references_downloaded_fromNCBI.fasta
-p     prefix to output filenames
-h     Print this Help.
##################################################

3 BLAST.sh --> script working, but not final --> in: OTUs from clustering step    out: Best hit BLAST table
in: _2f_representatives.fas/my_refs.uniq.fasta		out: your_project_local_blast.tab

4 perl Blast_results_format.pl --> format BLAST output to work in OTU table and stampadapt.sh
in: your_project_local_blast.tab		out: Blast_formatted2OTUtable.tab

###7_build_OTU_table###
vi build_OTU_table.sh
in: global derep fasta, clustering outputs, Blast_formatted2OTUtable.tab and quality file
out: the complete version of the OTU table.

###8_filter_table_phyloseq###
1. filter_OTU_table.sh   in: OTU table   out: OTU table formatted to load in Phyloseq
2. perl taxa_assignment_table.pl Blast_formatted2OTUtable.tab  out: final_OTU_tax_assignments.txt to load in Phyloseq
3. script for loading your tables in phyloseq and some first steps: metapipe_phyloseq.R

Second taxa assignment option:
###6_taxa_assign###
1 primers_cut.sh : Download your references from BOLD, cut it with your primers and format header with taxonomic lineage

2 ./stampadapt.sh -h
##################################################
Runs the adapted version of STAMPA LCA taxa assignment
Usage: ./stampadapt.sh [-q query.fasta] [-d references.fasta] [-o output] [-p pwd]
-q     OTUs fasta file, the output from clustering step
-d     references sequences formatted
-o     output name for the final assignments file.
-h     print this help
##################################################
3 stampaplot.sh : Just one example of how to plot STAMPA results, still in progress

